{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "If someone has an idea for a new classifier, it should compare the performance of its new classifier with the performance of standard classifiers on some standard datasets.\n",
    "\n",
    "But what are the performances of these standard classifiers on these standard datasets?\n",
    "\n",
    "This notebook tries to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_NR_TEST_IMAGES = 10000\n",
    "PARAM_NR_TRAIN_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data():\n",
    "    \n",
    "    # 1. load MNIST dataset\n",
    "    mnist_dataset = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist_dataset.load_data()\n",
    "    \n",
    "    # 2. map image pixel values from [0,255] to [0,1]\n",
    "    x_train = x_train.astype(float)\n",
    "    x_test  = x_test.astype(float)\n",
    "    \n",
    "    x_train = x_train * (1.0/255.0)\n",
    "    x_test  = x_test  * (1.0/255.0)\n",
    "    \n",
    "    # 3. images are 2D, prepare input as 1D input for a MLP\n",
    "    x_train = x_train.reshape(-1,28*28)\n",
    "    x_test  = x_test.reshape(-1,28*28)\n",
    "    \n",
    "    # 4. map train data target labels to one-hot encoded vectors\n",
    "    y_train_onehot = np.zeros((y_train.size, y_train.max()+1))\n",
    "    y_train_onehot[np.arange(y_train.size),y_train] = 1\n",
    "    \n",
    "    # 5. map test data target labels to one-hot encoded vectors\n",
    "    y_test_onehot = np.zeros((y_test.size, y_train.max()+1))\n",
    "    y_test_onehot[np.arange(y_test.size),y_test] = 1\n",
    "    \n",
    "    return x_train, y_train_onehot, \\\n",
    "           x_test,  y_test_onehot\n",
    "\n",
    "# get MNIST data now\n",
    "x_train, y_train, x_test, y_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for i in range(0,5):\n",
    "    plt.imshow( x_train[i,:].reshape(28,28), cmap=\"gray\" )\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(your_model, x_test, y_test, how_many_to_test=None, model_type=\"MLP\"):\n",
    "    \n",
    "    nr_test_samples = x_test.shape[0]\n",
    "    \n",
    "    if how_many_to_test is None:\n",
    "        how_many_to_test = nr_test_samples\n",
    "   \n",
    "    correct = 0\n",
    "    for sample_id in range(0, how_many_to_test):\n",
    "        \n",
    "        if sample_id % 100 == 0:\n",
    "            print(\"tested so far {0} of {1} images\".format(sample_id, how_many_to_test))\n",
    "\n",
    "        # get next test sample\n",
    "        input_vec   = x_test[sample_id]\n",
    "        teacher_vec = y_test[sample_id]\n",
    "        \n",
    "        # compute prediction vector\n",
    "                \n",
    "        if model_type == \"MLP\":\n",
    "            \n",
    "            # note: the input to a MLP is a batch of 1D inputs, so 2D!\n",
    "            pred_vec = your_model.predict( input_vec.reshape(1, -1) )\n",
    "            \n",
    "            \n",
    "        elif model_type == \"CNN\":\n",
    "            \n",
    "            # note: the input to a CNN is a batch of 3D inputs, so 3D!\n",
    "            # 1 test image\n",
    "            # 28 pixel wide\n",
    "            # 28 pixel high\n",
    "            # 1 color channel (gray-scale image)\n",
    "            # --> shape is (1,28,28,1)\n",
    "            pred_vec = your_model.predict( input_vec.reshape(1, 28,28,1) )\n",
    "        \n",
    "        # compute predicted label\n",
    "        pred_label = np.argmax( pred_vec )\n",
    "        \n",
    "        # get actual label\n",
    "        gt_label = np.argmax( teacher_vec )\n",
    "        \n",
    "        #print(\"pred: {0} vs. gt: {1}\".format(pred_label, gt_label))\n",
    "        \n",
    "        # was the predicted label correct?\n",
    "        if pred_label == gt_label:\n",
    "            correct += 1\n",
    "            \n",
    "    acc = correct/how_many_to_test\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nr_inputs = 28*28\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10,\n",
    "                             activation=\"linear\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=PARAM_NR_TRAIN_EPOCHS, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nr_inputs = 28*28\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(200,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.add(keras.layers.Dense(50,\n",
    "                             activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10,\n",
    "                             activation=\"linear\"))\n",
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=PARAM_NR_TRAIN_EPOCHS, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu', name=\"FC\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation='linear', name=\"output\"))\n",
    "\n",
    "model.compile(loss=losses.mean_squared_error, optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model,\n",
    "           x_test,\n",
    "           y_test,\n",
    "           how_many_to_test=500,\n",
    "           model_type=\"CNN\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train.reshape(60000, 28,28,1),\n",
    "                    y_train,\n",
    "                    epochs=PARAM_NR_TRAIN_EPOCHS/2,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "Why Fashion-MNIST?\n",
    "\n",
    "See https://github.com/zalandoresearch/fashion-mnist:\n",
    "\n",
    "Quote from the Fashion-MNIST GitHub website:\n",
    "\n",
    "    To Serious Machine Learning Researchers\n",
    "\n",
    "    Seriously, we are talking about replacing MNIST. Here are some good reasons:\n",
    "\n",
    "    MNIST is too easy. Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily. Check out our side-by-side benchmark for Fashion-MNIST vs. MNIST, and read \"Most pairs of MNIST digits can be distinguished pretty well by just one pixel.\"\n",
    "\n",
    "    MNIST is overused. In this April 2017 Twitter thread, Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\n",
    "\n",
    "    MNIST can not represent modern CV tasks, as noted in this April 2017 Twitter thread, deep learning expert/Keras author Fran√ßois Chollet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data():\n",
    "    \n",
    "    # 1. load FASHION MNIST dataset\n",
    "    fashionmnist_dataset = keras.datasets.fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashionmnist_dataset.load_data()\n",
    "    \n",
    "    # 2. map image pixel values from [0,255] to [0,1]\n",
    "    x_train = x_train.astype(float)\n",
    "    x_test  = x_test.astype(float)\n",
    "    \n",
    "    x_train = x_train * (1.0/255.0)\n",
    "    x_test  = x_test  * (1.0/255.0)\n",
    "    \n",
    "    # 3. images are 2D, prepare input as 1D input for a MLP\n",
    "    x_train = x_train.reshape(-1,28*28)\n",
    "    x_test  = x_test.reshape(-1,28*28)\n",
    "    \n",
    "    # 4. map train data target labels to one-hot encoded vectors\n",
    "    y_train_onehot = np.zeros((y_train.size, y_train.max()+1))\n",
    "    y_train_onehot[np.arange(y_train.size),y_train] = 1\n",
    "    \n",
    "    # 5. map test data target labels to one-hot encoded vectors\n",
    "    y_test_onehot = np.zeros((y_test.size, y_train.max()+1))\n",
    "    y_test_onehot[np.arange(y_test.size),y_test] = 1\n",
    "    \n",
    "    return x_train, y_train_onehot, \\\n",
    "           x_test,  y_test_onehot\n",
    "\n",
    "# get FASHION MNIST data now\n",
    "x_train, y_train, x_test, y_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nr_inputs = 28*28\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10,\n",
    "                             activation=\"linear\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=PARAM_NR_TRAIN_EPOCHS, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "nr_inputs = 28*28\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(200,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(nr_inputs,)))\n",
    "model.add(keras.layers.Dense(50,\n",
    "                             activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10,\n",
    "                             activation=\"linear\"))\n",
    "model.compile(optimizer='sgd',               \n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=PARAM_NR_TRAIN_EPOCHS, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(model, x_test, y_test, PARAM_NR_TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "FC (Dense)                   (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 37,562\n",
      "Trainable params: 37,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu', name=\"FC\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation='linear', name=\"output\"))\n",
    "\n",
    "model.compile(loss=losses.mean_squared_error, optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tested so far 0 of 500 images\n",
      "tested so far 100 of 500 images\n",
      "tested so far 200 of 500 images\n",
      "tested so far 300 of 500 images\n",
      "tested so far 400 of 500 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test(model,\n",
    "           x_test,\n",
    "           y_test,\n",
    "           how_many_to_test=500,\n",
    "           model_type=\"CNN\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/150\n",
      "54000/54000 [==============================] - 108s 2ms/sample - loss: 0.0885 - val_loss: 0.0851\n",
      "Epoch 2/150\n",
      "54000/54000 [==============================] - 108s 2ms/sample - loss: 0.0810 - val_loss: 0.0768\n",
      "Epoch 3/150\n",
      "54000/54000 [==============================] - 109s 2ms/sample - loss: 0.0725 - val_loss: 0.0686\n",
      "Epoch 4/150\n",
      "54000/54000 [==============================] - 113s 2ms/sample - loss: 0.0657 - val_loss: 0.0626\n",
      "Epoch 5/150\n",
      "54000/54000 [==============================] - 111s 2ms/sample - loss: 0.0603 - val_loss: 0.0575\n",
      "Epoch 6/150\n",
      "54000/54000 [==============================] - 114s 2ms/sample - loss: 0.0559 - val_loss: 0.0535\n",
      "Epoch 7/150\n",
      "54000/54000 [==============================] - 111s 2ms/sample - loss: 0.0524 - val_loss: 0.0503\n",
      "Epoch 8/150\n",
      "54000/54000 [==============================] - 115s 2ms/sample - loss: 0.0495 - val_loss: 0.0476\n",
      "Epoch 9/150\n",
      "54000/54000 [==============================] - 113s 2ms/sample - loss: 0.0471 - val_loss: 0.0454\n",
      "Epoch 10/150\n",
      "54000/54000 [==============================] - 104s 2ms/sample - loss: 0.0453 - val_loss: 0.0438\n",
      "Epoch 11/150\n",
      "54000/54000 [==============================] - 112s 2ms/sample - loss: 0.0438 - val_loss: 0.0424\n",
      "Epoch 12/150\n",
      "54000/54000 [==============================] - 111s 2ms/sample - loss: 0.0426 - val_loss: 0.0414\n",
      "Epoch 13/150\n",
      "54000/54000 [==============================] - 116s 2ms/sample - loss: 0.0416 - val_loss: 0.0405\n",
      "Epoch 14/150\n",
      "54000/54000 [==============================] - 116s 2ms/sample - loss: 0.0408 - val_loss: 0.0398\n",
      "Epoch 15/150\n",
      "54000/54000 [==============================] - 114s 2ms/sample - loss: 0.0401 - val_loss: 0.0391\n",
      "Epoch 16/150\n",
      "54000/54000 [==============================] - 118s 2ms/sample - loss: 0.0394 - val_loss: 0.0385\n",
      "Epoch 17/150\n",
      "54000/54000 [==============================] - 112s 2ms/sample - loss: 0.0388 - val_loss: 0.0378\n",
      "Epoch 18/150\n",
      "54000/54000 [==============================] - 116s 2ms/sample - loss: 0.0382 - val_loss: 0.0375\n",
      "Epoch 19/150\n",
      "54000/54000 [==============================] - 114s 2ms/sample - loss: 0.0377 - val_loss: 0.0369\n",
      "Epoch 20/150\n",
      "54000/54000 [==============================] - 110s 2ms/sample - loss: 0.0372 - val_loss: 0.0365\n",
      "Epoch 21/150\n",
      "54000/54000 [==============================] - 110s 2ms/sample - loss: 0.0367 - val_loss: 0.0359\n",
      "Epoch 22/150\n",
      "54000/54000 [==============================] - 116s 2ms/sample - loss: 0.0363 - val_loss: 0.0356\n",
      "Epoch 23/150\n",
      "54000/54000 [==============================] - 114s 2ms/sample - loss: 0.0359 - val_loss: 0.0354\n",
      "Epoch 24/150\n",
      "54000/54000 [==============================] - 120s 2ms/sample - loss: 0.0355 - val_loss: 0.0349\n",
      "Epoch 25/150\n",
      "54000/54000 [==============================] - 117s 2ms/sample - loss: 0.0352 - val_loss: 0.0345\n",
      "Epoch 26/150\n",
      "54000/54000 [==============================] - 116s 2ms/sample - loss: 0.0349 - val_loss: 0.0342\n",
      "Epoch 27/150\n",
      "54000/54000 [==============================] - 121s 2ms/sample - loss: 0.0345 - val_loss: 0.0343\n",
      "Epoch 28/150\n",
      "54000/54000 [==============================] - 111s 2ms/sample - loss: 0.0342 - val_loss: 0.0338\n",
      "Epoch 29/150\n",
      "54000/54000 [==============================] - 120s 2ms/sample - loss: 0.0340 - val_loss: 0.0334\n",
      "Epoch 30/150\n",
      "54000/54000 [==============================] - 119s 2ms/sample - loss: 0.0337 - val_loss: 0.0331\n",
      "Epoch 31/150\n",
      "54000/54000 [==============================] - 118s 2ms/sample - loss: 0.0334 - val_loss: 0.0329\n",
      "Epoch 32/150\n",
      "54000/54000 [==============================] - 117s 2ms/sample - loss: 0.0332 - val_loss: 0.0326\n",
      "Epoch 33/150\n",
      "54000/54000 [==============================] - 110s 2ms/sample - loss: 0.0330 - val_loss: 0.0328\n",
      "Epoch 34/150\n",
      "54000/54000 [==============================] - 118s 2ms/sample - loss: 0.0327 - val_loss: 0.0325\n",
      "Epoch 35/150\n",
      "54000/54000 [==============================] - 126s 2ms/sample - loss: 0.0325 - val_loss: 0.0325\n",
      "Epoch 36/150\n",
      "54000/54000 [==============================] - 114s 2ms/sample - loss: 0.0323 - val_loss: 0.0319\n",
      "Epoch 37/150\n",
      "54000/54000 [==============================] - 123s 2ms/sample - loss: 0.0321 - val_loss: 0.0317\n",
      "Epoch 38/150\n",
      "54000/54000 [==============================] - 115s 2ms/sample - loss: 0.0319 - val_loss: 0.0316\n",
      "Epoch 39/150\n",
      "54000/54000 [==============================] - 108s 2ms/sample - loss: 0.0317 - val_loss: 0.0314\n",
      "Epoch 40/150\n",
      "54000/54000 [==============================] - 106s 2ms/sample - loss: 0.0315 - val_loss: 0.0316\n",
      "Epoch 41/150\n",
      "54000/54000 [==============================] - 109s 2ms/sample - loss: 0.0314 - val_loss: 0.0312\n",
      "Epoch 42/150\n",
      "54000/54000 [==============================] - 102s 2ms/sample - loss: 0.0312 - val_loss: 0.0309\n",
      "Epoch 43/150\n",
      "54000/54000 [==============================] - 98s 2ms/sample - loss: 0.0310 - val_loss: 0.0310\n",
      "Epoch 44/150\n",
      "54000/54000 [==============================] - 111s 2ms/sample - loss: 0.0309 - val_loss: 0.0305\n",
      "Epoch 45/150\n",
      "54000/54000 [==============================] - 107s 2ms/sample - loss: 0.0307 - val_loss: 0.0306\n",
      "Epoch 46/150\n",
      "54000/54000 [==============================] - 102s 2ms/sample - loss: 0.0306 - val_loss: 0.0305\n",
      "Epoch 47/150\n",
      "54000/54000 [==============================] - 107s 2ms/sample - loss: 0.0304 - val_loss: 0.0302\n",
      "Epoch 48/150\n",
      "54000/54000 [==============================] - 104s 2ms/sample - loss: 0.0303 - val_loss: 0.0300\n",
      "Epoch 49/150\n",
      "54000/54000 [==============================] - 112s 2ms/sample - loss: 0.0301 - val_loss: 0.0300\n",
      "Epoch 50/150\n",
      "54000/54000 [==============================] - 107s 2ms/sample - loss: 0.0300 - val_loss: 0.0299\n",
      "Epoch 51/150\n",
      "54000/54000 [==============================] - 104s 2ms/sample - loss: 0.0299 - val_loss: 0.0297\n",
      "Epoch 52/150\n",
      "54000/54000 [==============================] - 105s 2ms/sample - loss: 0.0297 - val_loss: 0.0297\n",
      "Epoch 53/150\n",
      "54000/54000 [==============================] - 86s 2ms/sample - loss: 0.0296 - val_loss: 0.0295\n",
      "Epoch 54/150\n",
      "54000/54000 [==============================] - 89s 2ms/sample - loss: 0.0295 - val_loss: 0.0293\n",
      "Epoch 55/150\n",
      "54000/54000 [==============================] - 100s 2ms/sample - loss: 0.0294 - val_loss: 0.0292\n",
      "Epoch 56/150\n",
      "54000/54000 [==============================] - 88s 2ms/sample - loss: 0.0293 - val_loss: 0.0294\n",
      "Epoch 57/150\n",
      "54000/54000 [==============================] - 93s 2ms/sample - loss: 0.0291 - val_loss: 0.0291\n",
      "Epoch 58/150\n",
      "54000/54000 [==============================] - 93s 2ms/sample - loss: 0.0290 - val_loss: 0.0290\n",
      "Epoch 59/150\n",
      "54000/54000 [==============================] - 96s 2ms/sample - loss: 0.0289 - val_loss: 0.0290\n",
      "Epoch 60/150\n",
      "54000/54000 [==============================] - 91s 2ms/sample - loss: 0.0288 - val_loss: 0.0290\n",
      "Epoch 61/150\n",
      "54000/54000 [==============================] - 95s 2ms/sample - loss: 0.0287 - val_loss: 0.0286\n",
      "Epoch 62/150\n",
      "54000/54000 [==============================] - 91s 2ms/sample - loss: 0.0285 - val_loss: 0.0288\n",
      "Epoch 63/150\n",
      "54000/54000 [==============================] - 96s 2ms/sample - loss: 0.0284 - val_loss: 0.0285\n",
      "Epoch 64/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0283 - val_loss: 0.0285\n",
      "Epoch 65/150\n",
      "54000/54000 [==============================] - 99s 2ms/sample - loss: 0.0282 - val_loss: 0.0285\n",
      "Epoch 66/150\n",
      "54000/54000 [==============================] - 97s 2ms/sample - loss: 0.0281 - val_loss: 0.0281\n",
      "Epoch 67/150\n",
      "54000/54000 [==============================] - 96s 2ms/sample - loss: 0.0280 - val_loss: 0.0280\n",
      "Epoch 68/150\n",
      "54000/54000 [==============================] - 104s 2ms/sample - loss: 0.0279 - val_loss: 0.0285\n",
      "Epoch 69/150\n",
      "54000/54000 [==============================] - 94s 2ms/sample - loss: 0.0278 - val_loss: 0.0278\n",
      "Epoch 70/150\n",
      "54000/54000 [==============================] - 94s 2ms/sample - loss: 0.0277 - val_loss: 0.0283\n",
      "Epoch 71/150\n",
      "54000/54000 [==============================] - 94s 2ms/sample - loss: 0.0276 - val_loss: 0.0277\n",
      "Epoch 72/150\n",
      "54000/54000 [==============================] - 103s 2ms/sample - loss: 0.0275 - val_loss: 0.0275\n",
      "Epoch 73/150\n",
      "54000/54000 [==============================] - 91s 2ms/sample - loss: 0.0274 - val_loss: 0.0276\n",
      "Epoch 74/150\n",
      "54000/54000 [==============================] - 100s 2ms/sample - loss: 0.0273 - val_loss: 0.0273\n",
      "Epoch 75/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 83s 2ms/sample - loss: 0.0272 - val_loss: 0.0275\n",
      "Epoch 76/150\n",
      "54000/54000 [==============================] - 89s 2ms/sample - loss: 0.0271 - val_loss: 0.0273\n",
      "Epoch 77/150\n",
      "54000/54000 [==============================] - 88s 2ms/sample - loss: 0.0270 - val_loss: 0.0271\n",
      "Epoch 78/150\n",
      "54000/54000 [==============================] - 97s 2ms/sample - loss: 0.0269 - val_loss: 0.0270\n",
      "Epoch 79/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0268 - val_loss: 0.0270\n",
      "Epoch 80/150\n",
      "54000/54000 [==============================] - 76s 1ms/sample - loss: 0.0267 - val_loss: 0.0270\n",
      "Epoch 81/150\n",
      "54000/54000 [==============================] - 85s 2ms/sample - loss: 0.0267 - val_loss: 0.0269\n",
      "Epoch 82/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0266 - val_loss: 0.0271\n",
      "Epoch 83/150\n",
      "54000/54000 [==============================] - 82s 2ms/sample - loss: 0.0265 - val_loss: 0.0266\n",
      "Epoch 84/150\n",
      "54000/54000 [==============================] - 82s 2ms/sample - loss: 0.0264 - val_loss: 0.0265\n",
      "Epoch 85/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0263 - val_loss: 0.0271\n",
      "Epoch 86/150\n",
      "54000/54000 [==============================] - 89s 2ms/sample - loss: 0.0262 - val_loss: 0.0264\n",
      "Epoch 87/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0261 - val_loss: 0.0265\n",
      "Epoch 88/150\n",
      "54000/54000 [==============================] - 82s 2ms/sample - loss: 0.0260 - val_loss: 0.0263\n",
      "Epoch 89/150\n",
      "54000/54000 [==============================] - 84s 2ms/sample - loss: 0.0259 - val_loss: 0.0263\n",
      "Epoch 90/150\n",
      "54000/54000 [==============================] - 70s 1ms/sample - loss: 0.0259 - val_loss: 0.0262\n",
      "Epoch 91/150\n",
      "54000/54000 [==============================] - 78s 1ms/sample - loss: 0.0258 - val_loss: 0.0260\n",
      "Epoch 92/150\n",
      "54000/54000 [==============================] - 86s 2ms/sample - loss: 0.0257 - val_loss: 0.0259\n",
      "Epoch 93/150\n",
      "54000/54000 [==============================] - 78s 1ms/sample - loss: 0.0256 - val_loss: 0.0258\n",
      "Epoch 94/150\n",
      "54000/54000 [==============================] - 96s 2ms/sample - loss: 0.0256 - val_loss: 0.0257\n",
      "Epoch 95/150\n",
      "54000/54000 [==============================] - 77s 1ms/sample - loss: 0.0255 - val_loss: 0.0258\n",
      "Epoch 96/150\n",
      "54000/54000 [==============================] - 82s 2ms/sample - loss: 0.0254 - val_loss: 0.0256\n",
      "Epoch 97/150\n",
      "54000/54000 [==============================] - 94s 2ms/sample - loss: 0.0253 - val_loss: 0.0261\n",
      "Epoch 98/150\n",
      "54000/54000 [==============================] - 90s 2ms/sample - loss: 0.0252 - val_loss: 0.0256\n",
      "Epoch 99/150\n",
      "54000/54000 [==============================] - 73s 1ms/sample - loss: 0.0252 - val_loss: 0.0257\n",
      "Epoch 100/150\n",
      "54000/54000 [==============================] - 81s 1ms/sample - loss: 0.0251 - val_loss: 0.0258\n",
      "Epoch 101/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0250 - val_loss: 0.0255\n",
      "Epoch 102/150\n",
      "54000/54000 [==============================] - 91s 2ms/sample - loss: 0.0250 - val_loss: 0.0252\n",
      "Epoch 103/150\n",
      "54000/54000 [==============================] - 83s 2ms/sample - loss: 0.0249 - val_loss: 0.0254\n",
      "Epoch 104/150\n",
      "54000/54000 [==============================] - 92s 2ms/sample - loss: 0.0248 - val_loss: 0.0255\n",
      "Epoch 105/150\n",
      "54000/54000 [==============================] - 84s 2ms/sample - loss: 0.0247 - val_loss: 0.0251\n",
      "Epoch 106/150\n",
      "54000/54000 [==============================] - 76s 1ms/sample - loss: 0.0247 - val_loss: 0.0250\n",
      "Epoch 107/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 108/150\n",
      "54000/54000 [==============================] - 80s 1ms/sample - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 109/150\n",
      "54000/54000 [==============================] - 72s 1ms/sample - loss: 0.0245 - val_loss: 0.0248\n",
      "Epoch 110/150\n",
      "54000/54000 [==============================] - 91s 2ms/sample - loss: 0.0244 - val_loss: 0.0250\n",
      "Epoch 111/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0244 - val_loss: 0.0251\n",
      "Epoch 112/150\n",
      "54000/54000 [==============================] - 75s 1ms/sample - loss: 0.0243 - val_loss: 0.0247\n",
      "Epoch 113/150\n",
      "54000/54000 [==============================] - 81s 2ms/sample - loss: 0.0242 - val_loss: 0.0246\n",
      "Epoch 114/150\n",
      "54000/54000 [==============================] - 85s 2ms/sample - loss: 0.0242 - val_loss: 0.0247\n",
      "Epoch 115/150\n",
      "54000/54000 [==============================] - 85s 2ms/sample - loss: 0.0241 - val_loss: 0.0245\n",
      "Epoch 116/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0241 - val_loss: 0.0244\n",
      "Epoch 117/150\n",
      "54000/54000 [==============================] - 70s 1ms/sample - loss: 0.0240 - val_loss: 0.0246\n",
      "Epoch 118/150\n",
      "54000/54000 [==============================] - 78s 1ms/sample - loss: 0.0240 - val_loss: 0.0243\n",
      "Epoch 119/150\n",
      "54000/54000 [==============================] - 92s 2ms/sample - loss: 0.0239 - val_loss: 0.0243\n",
      "Epoch 120/150\n",
      "54000/54000 [==============================] - 93s 2ms/sample - loss: 0.0238 - val_loss: 0.0243\n",
      "Epoch 121/150\n",
      "54000/54000 [==============================] - 89s 2ms/sample - loss: 0.0238 - val_loss: 0.0242\n",
      "Epoch 122/150\n",
      "54000/54000 [==============================] - 84s 2ms/sample - loss: 0.0237 - val_loss: 0.0246\n",
      "Epoch 123/150\n",
      "54000/54000 [==============================] - 93s 2ms/sample - loss: 0.0237 - val_loss: 0.0243\n",
      "Epoch 124/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 125/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0236 - val_loss: 0.0245\n",
      "Epoch 126/150\n",
      "54000/54000 [==============================] - 89s 2ms/sample - loss: 0.0235 - val_loss: 0.0243\n",
      "Epoch 127/150\n",
      "54000/54000 [==============================] - 76s 1ms/sample - loss: 0.0235 - val_loss: 0.0242\n",
      "Epoch 128/150\n",
      "54000/54000 [==============================] - 88s 2ms/sample - loss: 0.0234 - val_loss: 0.0240\n",
      "Epoch 129/150\n",
      "54000/54000 [==============================] - 82s 2ms/sample - loss: 0.0234 - val_loss: 0.0242\n",
      "Epoch 130/150\n",
      "54000/54000 [==============================] - 104s 2ms/sample - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 131/150\n",
      "54000/54000 [==============================] - 77s 1ms/sample - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 132/150\n",
      "54000/54000 [==============================] - 97s 2ms/sample - loss: 0.0232 - val_loss: 0.0241\n",
      "Epoch 133/150\n",
      "54000/54000 [==============================] - 86s 2ms/sample - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 134/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0231 - val_loss: 0.0239\n",
      "Epoch 135/150\n",
      "54000/54000 [==============================] - 106s 2ms/sample - loss: 0.0231 - val_loss: 0.0239\n",
      "Epoch 136/150\n",
      "54000/54000 [==============================] - 84s 2ms/sample - loss: 0.0230 - val_loss: 0.0236\n",
      "Epoch 137/150\n",
      "54000/54000 [==============================] - 86s 2ms/sample - loss: 0.0230 - val_loss: 0.0236\n",
      "Epoch 138/150\n",
      "54000/54000 [==============================] - 78s 1ms/sample - loss: 0.0229 - val_loss: 0.0235\n",
      "Epoch 139/150\n",
      "54000/54000 [==============================] - 84s 2ms/sample - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 140/150\n",
      "54000/54000 [==============================] - 92s 2ms/sample - loss: 0.0229 - val_loss: 0.0237\n",
      "Epoch 141/150\n",
      "54000/54000 [==============================] - 78s 1ms/sample - loss: 0.0228 - val_loss: 0.0234\n",
      "Epoch 142/150\n",
      "54000/54000 [==============================] - 93s 2ms/sample - loss: 0.0228 - val_loss: 0.0238\n",
      "Epoch 143/150\n",
      "54000/54000 [==============================] - 83s 2ms/sample - loss: 0.0227 - val_loss: 0.0238\n",
      "Epoch 144/150\n",
      "54000/54000 [==============================] - 94s 2ms/sample - loss: 0.0227 - val_loss: 0.0233\n",
      "Epoch 145/150\n",
      "54000/54000 [==============================] - 85s 2ms/sample - loss: 0.0226 - val_loss: 0.0232\n",
      "Epoch 146/150\n",
      "54000/54000 [==============================] - 87s 2ms/sample - loss: 0.0226 - val_loss: 0.0232\n",
      "Epoch 147/150\n",
      "54000/54000 [==============================] - 79s 1ms/sample - loss: 0.0225 - val_loss: 0.0235\n",
      "Epoch 148/150\n",
      "54000/54000 [==============================] - 99s 2ms/sample - loss: 0.0225 - val_loss: 0.0235\n",
      "Epoch 149/150\n",
      "54000/54000 [==============================] - 90s 2ms/sample - loss: 0.0225 - val_loss: 0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/150\n",
      "54000/54000 [==============================] - 86s 2ms/sample - loss: 0.0224 - val_loss: 0.0231\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train.reshape(60000, 28,28,1),\n",
    "                    y_train,\n",
    "                    epochs=int(PARAM_NR_TRAIN_EPOCHS/2),\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tested so far 0 of 10000 images\n",
      "tested so far 100 of 10000 images\n",
      "tested so far 200 of 10000 images\n",
      "tested so far 300 of 10000 images\n",
      "tested so far 400 of 10000 images\n",
      "tested so far 500 of 10000 images\n",
      "tested so far 600 of 10000 images\n",
      "tested so far 700 of 10000 images\n",
      "tested so far 800 of 10000 images\n",
      "tested so far 900 of 10000 images\n",
      "tested so far 1000 of 10000 images\n",
      "tested so far 1100 of 10000 images\n",
      "tested so far 1200 of 10000 images\n",
      "tested so far 1300 of 10000 images\n",
      "tested so far 1400 of 10000 images\n",
      "tested so far 1500 of 10000 images\n",
      "tested so far 1600 of 10000 images\n",
      "tested so far 1700 of 10000 images\n",
      "tested so far 1800 of 10000 images\n",
      "tested so far 1900 of 10000 images\n",
      "tested so far 2000 of 10000 images\n",
      "tested so far 2100 of 10000 images\n",
      "tested so far 2200 of 10000 images\n",
      "tested so far 2300 of 10000 images\n",
      "tested so far 2400 of 10000 images\n",
      "tested so far 2500 of 10000 images\n",
      "tested so far 2600 of 10000 images\n",
      "tested so far 2700 of 10000 images\n",
      "tested so far 2800 of 10000 images\n",
      "tested so far 2900 of 10000 images\n",
      "tested so far 3000 of 10000 images\n",
      "tested so far 3100 of 10000 images\n",
      "tested so far 3200 of 10000 images\n",
      "tested so far 3300 of 10000 images\n",
      "tested so far 3400 of 10000 images\n",
      "tested so far 3500 of 10000 images\n",
      "tested so far 3600 of 10000 images\n",
      "tested so far 3700 of 10000 images\n",
      "tested so far 3800 of 10000 images\n",
      "tested so far 3900 of 10000 images\n",
      "tested so far 4000 of 10000 images\n",
      "tested so far 4100 of 10000 images\n",
      "tested so far 4200 of 10000 images\n",
      "tested so far 4300 of 10000 images\n",
      "tested so far 4400 of 10000 images\n",
      "tested so far 4500 of 10000 images\n",
      "tested so far 4600 of 10000 images\n",
      "tested so far 4700 of 10000 images\n",
      "tested so far 4800 of 10000 images\n",
      "tested so far 4900 of 10000 images\n",
      "tested so far 5000 of 10000 images\n",
      "tested so far 5100 of 10000 images\n",
      "tested so far 5200 of 10000 images\n",
      "tested so far 5300 of 10000 images\n",
      "tested so far 5400 of 10000 images\n",
      "tested so far 5500 of 10000 images\n",
      "tested so far 5600 of 10000 images\n",
      "tested so far 5700 of 10000 images\n",
      "tested so far 5800 of 10000 images\n",
      "tested so far 5900 of 10000 images\n",
      "tested so far 6000 of 10000 images\n",
      "tested so far 6100 of 10000 images\n",
      "tested so far 6200 of 10000 images\n",
      "tested so far 6300 of 10000 images\n",
      "tested so far 6400 of 10000 images\n",
      "tested so far 6500 of 10000 images\n",
      "tested so far 6600 of 10000 images\n",
      "tested so far 6700 of 10000 images\n",
      "tested so far 6800 of 10000 images\n",
      "tested so far 6900 of 10000 images\n",
      "tested so far 7000 of 10000 images\n",
      "tested so far 7100 of 10000 images\n",
      "tested so far 7200 of 10000 images\n",
      "tested so far 7300 of 10000 images\n",
      "tested so far 7400 of 10000 images\n",
      "tested so far 7500 of 10000 images\n",
      "tested so far 7600 of 10000 images\n",
      "tested so far 7700 of 10000 images\n",
      "tested so far 7800 of 10000 images\n",
      "tested so far 7900 of 10000 images\n",
      "tested so far 8000 of 10000 images\n",
      "tested so far 8100 of 10000 images\n",
      "tested so far 8200 of 10000 images\n",
      "tested so far 8300 of 10000 images\n",
      "tested so far 8400 of 10000 images\n",
      "tested so far 8500 of 10000 images\n",
      "tested so far 8600 of 10000 images\n",
      "tested so far 8700 of 10000 images\n",
      "tested so far 8800 of 10000 images\n",
      "tested so far 8900 of 10000 images\n",
      "tested so far 9000 of 10000 images\n",
      "tested so far 9100 of 10000 images\n",
      "tested so far 9200 of 10000 images\n",
      "tested so far 9300 of 10000 images\n",
      "tested so far 9400 of 10000 images\n",
      "tested so far 9500 of 10000 images\n",
      "tested so far 9600 of 10000 images\n",
      "tested so far 9700 of 10000 images\n",
      "tested so far 9800 of 10000 images\n",
      "tested so far 9900 of 10000 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8404"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test(model,\n",
    "           x_test,\n",
    "           y_test,\n",
    "           PARAM_NR_TEST_IMAGES,\n",
    "           model_type=\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model         | Dataset       | Acc test data  | Nr train epochs |\n",
    "|---------------|---------------|----------------|-----------------|\n",
    "| Perceptron    | MNIST         | 85%            | 300             |\n",
    "| MLP 200-50-10 | MNIST         | 97%            | 300             |\n",
    "| CNN           | MNIST         |                | 150             |\n",
    "| Perceptron    | Fashion-MNIST | 81%            | 300             |\n",
    "| MLP 200-50-10 | Fashion-MNIST | 88%            | 300             |\n",
    "| CNN           | Fashion-MNIST | 84%            | 150             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360.167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
